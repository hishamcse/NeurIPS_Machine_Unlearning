{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "422d71c9",
   "metadata": {
    "papermill": {
     "duration": 0.018242,
     "end_time": "2024-02-21T06:19:57.410239",
     "exception": false,
     "start_time": "2024-02-21T06:19:57.391997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab1d5ab",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-21T06:19:57.447165Z",
     "iopub.status.busy": "2024-02-21T06:19:57.446839Z",
     "iopub.status.idle": "2024-02-21T06:20:02.935576Z",
     "shell.execute_reply": "2024-02-21T06:20:02.934748Z"
    },
    "papermill": {
     "duration": 5.510343,
     "end_time": "2024-02-21T06:20:02.938019",
     "exception": false,
     "start_time": "2024-02-21T06:19:57.427676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import requests\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, model_selection\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e203e468",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:02.975683Z",
     "iopub.status.busy": "2024-02-21T06:20:02.974915Z",
     "iopub.status.idle": "2024-02-21T06:20:02.979645Z",
     "shell.execute_reply": "2024-02-21T06:20:02.978772Z"
    },
    "papermill": {
     "duration": 0.025523,
     "end_time": "2024-02-21T06:20:02.981638",
     "exception": false,
     "start_time": "2024-02-21T06:20:02.956115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It's really important to add an accelerator to your notebook, as otherwise the submission will fail.\n",
    "# We recomment using the P100 GPU rather than T4 as it's faster and will increase the chances of passing the time cut-off threshold.\n",
    "\n",
    "if DEVICE != 'cuda':\n",
    "    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3793bf1a",
   "metadata": {
    "papermill": {
     "duration": 0.017356,
     "end_time": "2024-02-21T06:20:03.016351",
     "exception": false,
     "start_time": "2024-02-21T06:20:02.998995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Custom Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5508bdc",
   "metadata": {
    "papermill": {
     "duration": 0.017259,
     "end_time": "2024-02-21T06:20:03.050963",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.033704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Seed Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a91c46bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:03.087089Z",
     "iopub.status.busy": "2024-02-21T06:20:03.086355Z",
     "iopub.status.idle": "2024-02-21T06:20:03.099472Z",
     "shell.execute_reply": "2024-02-21T06:20:03.098586Z"
    },
    "papermill": {
     "duration": 0.033278,
     "end_time": "2024-02-21T06:20:03.101405",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.068127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9bf0bfec70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(3047)\n",
    "\n",
    "G_retain = torch.Generator()\n",
    "G_retain.manual_seed(3047)\n",
    "\n",
    "G_forget = torch.Generator()\n",
    "G_forget.manual_seed(3049)\n",
    "\n",
    "G_validate = torch.Generator()\n",
    "G_validate.manual_seed(30470)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca98ea",
   "metadata": {
    "papermill": {
     "duration": 0.017422,
     "end_time": "2024-02-21T06:20:03.136349",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.118927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Testing Version or Submission Version "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf0a320",
   "metadata": {
    "papermill": {
     "duration": 0.017007,
     "end_time": "2024-02-21T06:20:03.170534",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.153527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here at the time of **testing**, we will keep the **internet on**, set **test=True** and load pretrained cifar10 model to test our unlearning algorithm implementation. In this case, we will utilize some parts from the given starting kit by the competition organizers.<br>\n",
    "**Link:** https://github.com/unlearning-challenge/starting-kit/blob/main/unlearning-CIFAR10.ipynb <br><br>\n",
    "At the time of **submission**, **internet off** and **test=False**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8bee4a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:03.207075Z",
     "iopub.status.busy": "2024-02-21T06:20:03.206258Z",
     "iopub.status.idle": "2024-02-21T06:20:03.210519Z",
     "shell.execute_reply": "2024-02-21T06:20:03.209621Z"
    },
    "papermill": {
     "duration": 0.02458,
     "end_time": "2024-02-21T06:20:03.212430",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.187850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cea94e3",
   "metadata": {
    "papermill": {
     "duration": 0.017048,
     "end_time": "2024-02-21T06:20:03.246639",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.229591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fbc9285",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:03.283296Z",
     "iopub.status.busy": "2024-02-21T06:20:03.282485Z",
     "iopub.status.idle": "2024-02-21T06:20:03.296658Z",
     "shell.execute_reply": "2024-02-21T06:20:03.295790Z"
    },
    "papermill": {
     "duration": 0.034529,
     "end_time": "2024-02-21T06:20:03.298718",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.264189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions for loading the CIFAR10 dataset.\n",
    "\n",
    "if test:\n",
    "    \n",
    "    # The directory for a dataset and a pretrained model\n",
    "    test_dir = './test'\n",
    "    test_model_path = os.path.join(test_dir, \"weights_resnet18_cifar10.pth\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    \n",
    "    class PublicDataset(Dataset):\n",
    "        \n",
    "        def __init__(self, ds: Dataset):\n",
    "            self._ds = ds\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self._ds)\n",
    "    \n",
    "        def __getitem__(self, index):\n",
    "            item = self._ds[index]\n",
    "            result = {\n",
    "                'image': item[0],\n",
    "                'image_id': index,\n",
    "                'age_group': item[1],\n",
    "                'age': item[1],\n",
    "                'person_id': index,\n",
    "            }\n",
    "            return result\n",
    "    \n",
    "    def get_dataset(batch_size, thinning_param: int=1, root=test_dir) -> tuple[DataLoader, DataLoader, DataLoader, DataLoader, DataLoader]:\n",
    "        \n",
    "        # utils\n",
    "        normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        # create dataset\n",
    "        train_set = torchvision.datasets.CIFAR10(root=test_dir, train=True, download=True, transform=normalize)\n",
    "        train_ds = PublicDataset(train_set)\n",
    "        \n",
    "        # download the forget and retain index split\n",
    "        local_path = \"forget_idx.npy\"\n",
    "        if not os.path.exists(local_path):\n",
    "            response = requests.get(\n",
    "                \"https://storage.googleapis.com/unlearning-challenge/\" + local_path\n",
    "            )\n",
    "            open(local_path, \"wb\").write(response.content)\n",
    "            \n",
    "        forget_idx = np.load(local_path)\n",
    "\n",
    "        # construct indices of retain from those of the forget set\n",
    "        forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
    "        forget_mask[forget_idx] = True\n",
    "        retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
    "        \n",
    "        # split train set into a forget and a retain set\n",
    "        forget_ds = Subset(train_ds, forget_idx)\n",
    "        retain_ds = Subset(train_ds, retain_idx)\n",
    "        \n",
    "        full_val_set = torchvision.datasets.CIFAR10(root=test_dir, train=False, download=True, transform=normalize)\n",
    "        \n",
    "        test_set, val_set = torch.utils.data.random_split(full_val_set, [0.5, 0.5])\n",
    "        \n",
    "        val_ds = PublicDataset(val_set)\n",
    "        test_ds = PublicDataset(test_set)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n",
    "        forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n",
    "        validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        return train_loader, retain_loader, forget_loader, validation_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2547b5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:03.379687Z",
     "iopub.status.busy": "2024-02-21T06:20:03.379095Z",
     "iopub.status.idle": "2024-02-21T06:20:03.390982Z",
     "shell.execute_reply": "2024-02-21T06:20:03.390260Z"
    },
    "papermill": {
     "duration": 0.032962,
     "end_time": "2024-02-21T06:20:03.392971",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.360009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions for loading the hidden dataset.\n",
    "\n",
    "if not test:\n",
    "    \n",
    "    def load_example(df_row):\n",
    "        image = torchvision.io.read_image(df_row['image_path'])\n",
    "        result = {\n",
    "            'image': image,\n",
    "            'image_id': df_row['image_id'],\n",
    "            'age_group': df_row['age_group'],\n",
    "            'age': df_row['age'],\n",
    "            'person_id': df_row['person_id']\n",
    "        }\n",
    "        return result\n",
    "\n",
    "\n",
    "    class HiddenDataset(Dataset):\n",
    "        '''The hidden dataset.'''\n",
    "        def __init__(self, split='train'):\n",
    "            super().__init__()\n",
    "            self.examples = []\n",
    "\n",
    "            df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n",
    "            df['image_path'] = df['image_id'].apply(\n",
    "                lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n",
    "            df = df.sort_values(by='image_path')\n",
    "            df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n",
    "            if len(self.examples) == 0:\n",
    "                raise ValueError('No examples.')\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.examples)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            example = self.examples[idx]\n",
    "            image = example['image']\n",
    "            image = image.to(torch.float32)\n",
    "            example['image'] = image\n",
    "            return example\n",
    "\n",
    "\n",
    "    def get_dataset(batch_size):\n",
    "        '''Get the dataset.'''\n",
    "        retain_ds = HiddenDataset(split='retain')\n",
    "        forget_ds = HiddenDataset(split='forget')\n",
    "        val_ds = HiddenDataset(split='validation')\n",
    "\n",
    "        retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True, generator=G_retain)\n",
    "        forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True, generator=G_forget)\n",
    "        validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True, generator=G_validate)\n",
    "\n",
    "        return retain_loader, forget_loader, validation_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3df6d9",
   "metadata": {
    "papermill": {
     "duration": 0.017116,
     "end_time": "2024-02-21T06:20:03.427678",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.410562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Unlearning Algorithm (1st Place)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dd94a0",
   "metadata": {
    "papermill": {
     "duration": 0.017283,
     "end_time": "2024-02-21T06:20:03.462884",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.445601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "https://www.kaggle.com/competitions/neurips-2023-machine-unlearning/discussion/458721"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b31c2",
   "metadata": {
    "papermill": {
     "duration": 0.017873,
     "end_time": "2024-02-21T06:20:03.498314",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.480441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Kullback-Leibler Divergence Loss \n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss\n",
    "\n",
    "Used in the **1st stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d37f92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:03.535907Z",
     "iopub.status.busy": "2024-02-21T06:20:03.535231Z",
     "iopub.status.idle": "2024-02-21T06:20:03.541126Z",
     "shell.execute_reply": "2024-02-21T06:20:03.540247Z"
    },
    "papermill": {
     "duration": 0.026808,
     "end_time": "2024-02-21T06:20:03.543146",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.516338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kl_loss_sym(x,y):\n",
    "    \"\"\"\n",
    "    Calculates a symmetric version of the Kullback-Leibler (KL) divergence loss \n",
    "    between two input probability distributions. This version ensures symmetry by\n",
    "    summing the KL divergence calculated in both directions.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor representing a probability distribution.\n",
    "        y (torch.Tensor): Target tensor representing another probability distribution.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The symmetric KL divergence loss.\n",
    "    \"\"\"\n",
    "        \n",
    "    kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "    return kl_loss(nn.LogSoftmax(dim=-1)(x),y) + 0.85 * kl_loss(y.log(),nn.Softmax(dim=-1)(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c64bfca",
   "metadata": {
    "papermill": {
     "duration": 0.017249,
     "end_time": "2024-02-21T06:20:03.578026",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.560777",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Jensen Shennon Divergence(JSD) Loss \n",
    "\n",
    "https://discuss.pytorch.org/t/jensen-shannon-divergence/2626/10<br>\n",
    "https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
    "\n",
    "Used instead of **KL-Div Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1acc9f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:03.613902Z",
     "iopub.status.busy": "2024-02-21T06:20:03.613587Z",
     "iopub.status.idle": "2024-02-21T06:20:03.619386Z",
     "shell.execute_reply": "2024-02-21T06:20:03.618439Z"
    },
    "papermill": {
     "duration": 0.026099,
     "end_time": "2024-02-21T06:20:03.621435",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.595336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jsd_loss(x, y):\n",
    "    \"\"\"\n",
    "    Calculates the Jensen-Shannon Divergence (JSD) loss between two input \n",
    "    probability distributions. JSD is a symmetric measure of similarity between\n",
    "    distributions.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor representing a probability distribution.\n",
    "        y (torch.Tensor): Target tensor representing another probability distribution.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The calculated JSD loss.\n",
    "    \"\"\" \n",
    "    \n",
    "    # Softmax for normalization\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    x = softmax(x)\n",
    "    y = softmax(y)\n",
    "\n",
    "    # Average of the distributions\n",
    "    m = 0.5 * (x + y)\n",
    "\n",
    "    return 0.5 * (kl_loss_sym(x, m) + kl_loss_sym(y, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f674e5",
   "metadata": {
    "papermill": {
     "duration": 0.017058,
     "end_time": "2024-02-21T06:20:03.655824",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.638766",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Contrastive Learning Loss\n",
    "\n",
    "Used in the **second stage forget round**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a76124f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:03.691998Z",
     "iopub.status.busy": "2024-02-21T06:20:03.691438Z",
     "iopub.status.idle": "2024-02-21T06:20:03.696422Z",
     "shell.execute_reply": "2024-02-21T06:20:03.695537Z"
    },
    "papermill": {
     "duration": 0.02544,
     "end_time": "2024-02-21T06:20:03.698469",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.673029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tau = temperature co-efficient\n",
    "def cl_loss(outputs_forget, outputs_retain, tau=1.15):\n",
    "    return (-1.0 * nn.LogSoftmax(dim=-1)(outputs_forget @ outputs_retain.T/tau)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a6566",
   "metadata": {
    "papermill": {
     "duration": 0.016909,
     "end_time": "2024-02-21T06:20:03.732923",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.716014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2 Stage Training For Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1703c070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:03.768438Z",
     "iopub.status.busy": "2024-02-21T06:20:03.768172Z",
     "iopub.status.idle": "2024-02-21T06:20:03.782435Z",
     "shell.execute_reply": "2024-02-21T06:20:03.781643Z"
    },
    "papermill": {
     "duration": 0.034146,
     "end_time": "2024-02-21T06:20:03.784300",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.750154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this will run at the time of submission\n",
    "\n",
    "if not test:\n",
    "\n",
    "    def unlearning(net, retain_loader, forget_loader, val_loader):\n",
    "\n",
    "        # First Stage: Forgetting Stage (KL-Divergence Optimization)\n",
    "        epochs = 1\n",
    "        criterion = kl_loss_sym\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9, weight_decay=0)\n",
    "        scheduler = None      # as just 1 epoch\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            net.train()\n",
    "            for sample in forget_loader:\n",
    "                inputs = sample[\"image\"]\n",
    "                inputs = inputs.to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "    \n",
    "                uniform_psedo_label = torch.ones_like(outputs).to(DEVICE) / outputs.shape[1]            \n",
    "                loss = criterion(outputs, uniform_psedo_label)\n",
    "            \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                \n",
    "        # Second Stage: Adverserial Fine Tuning (1. Forget Round , 2. Retain Round)\n",
    "        epochs = 8\n",
    "        retain_batch_size = 256\n",
    "        criterion_forget = cl_loss\n",
    "        criterion_retain = nn.CrossEntropyLoss()\n",
    "        optimizer_forget = optim.SGD(net.parameters(), lr=3e-4, momentum=0.9, weight_decay=0)\n",
    "        optimizer_retain = optim.SGD(net.parameters(), lr=0.001 * retain_batch_size / 64 , \n",
    "                                     momentum=0.9, weight_decay=0.01)\n",
    "        scheduler_forget = optim.lr_scheduler.CosineAnnealingLR(optimizer_forget, \n",
    "                                                         T_max=epochs*len(forget_loader), eta_min=1e-6)\n",
    "        \n",
    "        \n",
    "        # generate unexpected random shuffling at retain set (using seed for stabilization)\n",
    "        # two set (1 for forget round, another for retain round)\n",
    "            \n",
    "        retain_loader_forget = DataLoader(retain_loader.dataset, batch_size=retain_batch_size, \n",
    "                                          shuffle=True)\n",
    "        retain_loader_retain = DataLoader(retain_loader.dataset, batch_size=retain_batch_size, \n",
    "                                          shuffle=True)\n",
    "\n",
    "        net.train()\n",
    "        \n",
    "        for ep in range(epochs):\n",
    "            net.train()\n",
    "            \n",
    "            # forget round\n",
    "            for _ in range(1):    # modification\n",
    "                for sample_forget, sample_retain in zip(forget_loader, retain_loader_forget):\n",
    "                    inputs_forget, inputs_retain = sample_forget[\"image\"], sample_retain[\"image\"]\n",
    "                    inputs_forget, inputs_retain = inputs_forget.to(DEVICE), inputs_retain.to(DEVICE)\n",
    "\n",
    "                    optimizer_forget.zero_grad()\n",
    "                    outputs_forget, outputs_retain = net(inputs_forget), net(inputs_retain).detach()\n",
    "\n",
    "                    # contrastive learning loss\n",
    "                    loss = criterion_forget(outputs_forget, outputs_retain)\n",
    "                    loss.backward()\n",
    "                    optimizer_forget.step()\n",
    "\n",
    "                    scheduler_forget.step()\n",
    "                \n",
    "            # retain round\n",
    "            for sample in retain_loader_retain:\n",
    "                inputs, labels = sample[\"image\"], sample[\"age_group\"]\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "                optimizer_retain.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                \n",
    "                # cross entropy loss\n",
    "                loss = criterion_retain(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer_retain.step()\n",
    "                \n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f1d49",
   "metadata": {
    "papermill": {
     "duration": 0.016994,
     "end_time": "2024-02-21T06:20:03.818637",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.801643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation Using Loss & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "331d4f64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:03.854689Z",
     "iopub.status.busy": "2024-02-21T06:20:03.854409Z",
     "iopub.status.idle": "2024-02-21T06:20:03.860851Z",
     "shell.execute_reply": "2024-02-21T06:20:03.859957Z"
    },
    "papermill": {
     "duration": 0.026436,
     "end_time": "2024-02-21T06:20:03.862637",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.836201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_acc_loss(net, dataloader, criterion, device = 'cuda'):\n",
    "    net.eval()\n",
    "    total_samp = 0\n",
    "    total_acc = 0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for sample in dataloader:\n",
    "        images, labels = sample['image'].to(device), sample['age_group'].to(device)\n",
    "        _pred = net(images)\n",
    "        total_samp += len(labels)\n",
    "        loss = criterion(_pred, labels)\n",
    "        total_loss += loss.item()\n",
    "        total_acc += (_pred.max(1)[1] == labels).float().sum().item()\n",
    "\n",
    "    mean_loss = total_loss / len(dataloader)\n",
    "    mean_acc = total_acc / total_samp * 100.0\n",
    "    \n",
    "    return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58418787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:03.899259Z",
     "iopub.status.busy": "2024-02-21T06:20:03.898764Z",
     "iopub.status.idle": "2024-02-21T06:20:03.919784Z",
     "shell.execute_reply": "2024-02-21T06:20:03.918865Z"
    },
    "papermill": {
     "duration": 0.041996,
     "end_time": "2024-02-21T06:20:03.921894",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.879898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this will be run at the time of testing\n",
    "# same algorithm as before; just some extra validation and print the results to check algorithm\n",
    "\n",
    "if test:\n",
    "    \n",
    "    def unlearning(net, retain_loader, forget_loader, val_loader):\n",
    "        \n",
    "        print(\"----------------------------------\")\n",
    "        \n",
    "        col_names = [\"Stage-Epoch\", \"Set Type\", \"Loss(%)\", \"Accuracy(%)\"]\n",
    "        table = []\n",
    "        \n",
    "        # test criterion\n",
    "        criterion_test = nn.CrossEntropyLoss()\n",
    "\n",
    "        # First Stage: KL-Divergence Optimization\n",
    "        epochs = 1\n",
    "        criterion = jsd_loss\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9, weight_decay=0)\n",
    "        scheduler = None      # as just 1 epoch\n",
    "        \n",
    "        # testing\n",
    "        net.eval()\n",
    "        l, a = calculate_acc_loss(net, forget_loader, criterion_test)\n",
    "        table.append([\"Before 1st Stage\", \"Forget\", l, a])\n",
    "        l, a = calculate_acc_loss(net, validation_loader, criterion_test)\n",
    "        table.append([\"Before 1st Stage\", \"Valid\", l, a])\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            net.train()\n",
    "            for sample in forget_loader:\n",
    "                inputs = sample[\"image\"]\n",
    "                inputs = inputs.to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "    \n",
    "                uniform_psedo_label = torch.ones_like(outputs).to(DEVICE) / outputs.shape[1]            \n",
    "                loss = criterion(outputs, uniform_psedo_label)\n",
    "                        \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "        # testing\n",
    "        l, a = calculate_acc_loss(net, forget_loader, criterion_test)\n",
    "        table.append([\"After 1st Stage\", \"Forget\", l, a])\n",
    "        l, a = calculate_acc_loss(net, validation_loader, criterion_test)\n",
    "        table.append([\"After 1st Stage\", \"Valid\", l, a])\n",
    "\n",
    "                \n",
    "        # Second Stage: Adverserial Fine Tuning (1. Forget Round , 2. Retain Round)\n",
    "        epochs = 8\n",
    "        retain_batch_size = 256\n",
    "        criterion_forget = cl_loss\n",
    "        criterion_retain = nn.CrossEntropyLoss()\n",
    "        optimizer_forget = optim.SGD(net.parameters(), lr=3e-4, momentum=0.9, weight_decay=0)\n",
    "        optimizer_retain = optim.SGD(net.parameters(), lr=0.001 * retain_batch_size / 64 , \n",
    "                                     momentum=0.9, weight_decay=0.01)\n",
    "        scheduler_forget = optim.lr_scheduler.CosineAnnealingLR(optimizer_forget, \n",
    "                                                         T_max=epochs*len(forget_loader), eta_min=1e-6)\n",
    "        \n",
    "        \n",
    "        # generate unexpected random shuffling at retain set\n",
    "        # two set (1 for forget round, another for retain round)\n",
    "            \n",
    "        retain_loader_forget = DataLoader(retain_loader.dataset, batch_size=retain_batch_size, \n",
    "                                          shuffle=True)\n",
    "        retain_loader_retain = DataLoader(retain_loader.dataset, batch_size=retain_batch_size, \n",
    "                                          shuffle=True)\n",
    "\n",
    "        net.train()\n",
    "        \n",
    "        for ep in tqdm.trange(epochs):\n",
    "            net.train()\n",
    "            \n",
    "            # forget round\n",
    "            for _ in range(1):    # modification\n",
    "                for sample_forget, sample_retain in zip(forget_loader, retain_loader_forget):\n",
    "                    inputs_forget, inputs_retain = sample_forget[\"image\"], sample_retain[\"image\"]\n",
    "                    inputs_forget, inputs_retain = inputs_forget.to(DEVICE), inputs_retain.to(DEVICE)\n",
    "\n",
    "                    optimizer_forget.zero_grad()\n",
    "                    outputs_forget, outputs_retain = net(inputs_forget), net(inputs_retain).detach()\n",
    "\n",
    "                    # contrastive learning loss\n",
    "                    loss = criterion_forget(outputs_forget, outputs_retain)\n",
    "                    loss.backward()\n",
    "                    optimizer_forget.step()\n",
    "\n",
    "                    scheduler_forget.step()\n",
    "                \n",
    "            # retain round\n",
    "            for sample in retain_loader_retain:\n",
    "                inputs, labels = sample[\"image\"], sample[\"age_group\"]\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "                optimizer_retain.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                                \n",
    "                loss = criterion_retain(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer_retain.step()\n",
    "                                                \n",
    "            # testing\n",
    "            stage_epoch = \"Stage 2 - Epoch \" + str(ep)\n",
    "            l, a = calculate_acc_loss(net, forget_loader, criterion_test)\n",
    "            table.append([stage_epoch, \"Forget\", l, a])\n",
    "            l, a = calculate_acc_loss(net, retain_loader_retain, criterion_test)\n",
    "            table.append([stage_epoch, \"Retain\", l, a])\n",
    "            l, a = calculate_acc_loss(net, validation_loader, criterion_test)\n",
    "            table.append([stage_epoch, \"Valid\", l, a])\n",
    "            \n",
    "        print(tabulate(table, headers=col_names, tablefmt=\"fancy_grid\"))\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b358981",
   "metadata": {
    "papermill": {
     "duration": 0.017305,
     "end_time": "2024-02-21T06:20:03.956927",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.939622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loss Acc Evaluation & Test Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af47fc68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:03.993436Z",
     "iopub.status.busy": "2024-02-21T06:20:03.992642Z",
     "iopub.status.idle": "2024-02-21T06:20:04.000064Z",
     "shell.execute_reply": "2024-02-21T06:20:03.999191Z"
    },
    "papermill": {
     "duration": 0.027776,
     "end_time": "2024-02-21T06:20:04.002037",
     "exception": false,
     "start_time": "2024-02-21T06:20:03.974261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    \n",
    "    n_checkpoints = 1  # in the submission, there will be 512 points\n",
    "    \n",
    "    if not os.path.exists(test_model_path):\n",
    "        response = requests.get(\n",
    "            \"https://storage.googleapis.com/unlearning-challenge/weights_resnet18_cifar10.pth\")\n",
    "        open(test_model_path, \"wb\").write(response.content)    \n",
    "    \n",
    "    os.makedirs('/kaggle/tmp', exist_ok=True)\n",
    "    random.seed(42)   # just for reproducibality\n",
    "        \n",
    "    train_loader, retain_loader, forget_loader, validation_loader, test_loader = get_dataset(64)\n",
    "    net = resnet18(weights=None, num_classes=10)\n",
    "    net.to(DEVICE)\n",
    "    for i in tqdm.trange(n_checkpoints):\n",
    "        net.load_state_dict(torch.load(test_model_path))\n",
    "        net_ = unlearning(net, retain_loader, forget_loader, validation_loader)\n",
    "        state = net_.state_dict()\n",
    "        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n",
    "\n",
    "#     # Ensure that submission.zip will contain exactly n_checkpoints \n",
    "#     unlearned_ckpts = os.listdir('/kaggle/tmp')\n",
    "#     if len(unlearned_ckpts) != n_checkpoints:\n",
    "#         raise RuntimeError('The submission will throw an exception otherwise.')\n",
    "\n",
    "#     subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2aeadf",
   "metadata": {
    "papermill": {
     "duration": 0.017672,
     "end_time": "2024-02-21T06:20:04.037225",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.019553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Comparison With Trained Model Exclusively on Retain Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1bf01f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:04.075045Z",
     "iopub.status.busy": "2024-02-21T06:20:04.074215Z",
     "iopub.status.idle": "2024-02-21T06:20:04.085193Z",
     "shell.execute_reply": "2024-02-21T06:20:04.084208Z"
    },
    "papermill": {
     "duration": 0.031827,
     "end_time": "2024-02-21T06:20:04.087351",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.055524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    \n",
    "    # download weights of a model trained exclusively on the retain set\n",
    "    local_path = \"retrain_weights_resnet18_cifar10.pth\"\n",
    "    if not os.path.exists(local_path):\n",
    "        response = requests.get(\n",
    "            \"https://storage.googleapis.com/unlearning-challenge/\" + local_path\n",
    "        )\n",
    "        open(local_path, \"wb\").write(response.content)\n",
    "\n",
    "    weights_pretrained = torch.load(local_path, map_location=DEVICE)\n",
    "\n",
    "    # load model with pre-trained weights\n",
    "    rt_model = resnet18(weights=None, num_classes=10)\n",
    "    rt_model.load_state_dict(weights_pretrained)\n",
    "    rt_model.to(DEVICE)\n",
    "    rt_model.eval()\n",
    "\n",
    "    # test criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    net = net_\n",
    "\n",
    "    print(\"------------ Exclusive Retrained Model---------------\")\n",
    "    l, a = calculate_acc_loss(rt_model, retain_loader, criterion)\n",
    "    print(f\"Retain set accuracy: {a:0.2f}%\")\n",
    "    print(f\"Retain set loss: {l:0.2f}\")\n",
    "    l, a = calculate_acc_loss(rt_model, forget_loader, criterion)\n",
    "    print(f\"Forget set accuracy: {a:0.2f}%\")\n",
    "    print(f\"Forget set loss: {l:0.2f}\")\n",
    "    l, a = calculate_acc_loss(rt_model, validation_loader, criterion)\n",
    "    print(f\"Validation set accuracy: {a:0.2f}%\")\n",
    "    print(f\"Validation set loss: {l:0.2f}\")\n",
    "    l, a = calculate_acc_loss(rt_model, test_loader, criterion)\n",
    "    print(f\"Test set accuracy: {a:0.2f}%\")\n",
    "    print(f\"Test set loss: {l:0.2f}\")\n",
    "        \n",
    "    print(\"------------ Unlearned Model ---------------\")\n",
    "    l, a = calculate_acc_loss(net, retain_loader, criterion)\n",
    "    print(f\"Retain set accuracy: {a:0.2f}%\")\n",
    "    print(f\"Retain set loss: {l:0.2f}\")\n",
    "    l, a = calculate_acc_loss(net, forget_loader, criterion)\n",
    "    print(f\"Forget set accuracy: {a:0.2f}%\")\n",
    "    print(f\"Forget set loss: {l:0.2f}\")\n",
    "    l, a = calculate_acc_loss(net, validation_loader, criterion)\n",
    "    print(f\"Validation set accuracy: {a:0.2f}%\")\n",
    "    print(f\"Validation set loss: {l:0.2f}\")\n",
    "    l, a = calculate_acc_loss(net, test_loader, criterion)\n",
    "    print(f\"Test set accuracy: {a:0.2f}%\")\n",
    "    print(f\"Test set loss: {l:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4644a1df",
   "metadata": {
    "papermill": {
     "duration": 0.017204,
     "end_time": "2024-02-21T06:20:04.122317",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.105113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation using MIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2549cd",
   "metadata": {
    "papermill": {
     "duration": 0.017194,
     "end_time": "2024-02-21T06:20:04.157067",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.139873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Reference:** https://github.com/unlearning-challenge/starting-kit/blob/main/unlearning-CIFAR10.ipynb<br>\n",
    "We will evaluate the trained models using Simple Membership Inference Attacks(MIA). This is **not used** as evaluation metric for the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69d09e",
   "metadata": {
    "papermill": {
     "duration": 0.017285,
     "end_time": "2024-02-21T06:20:04.191755",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.174470",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This MIA consists of a **logistic regression model** that predicts whether the model was trained on a particular sample from that sample's loss. To get an idea on the difficulty of this problem, we first plot below a histogram of the losses of the pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e07051d",
   "metadata": {
    "papermill": {
     "duration": 0.017067,
     "end_time": "2024-02-21T06:20:04.226224",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.209157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Visualize Pre-trained Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66361a5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:04.263352Z",
     "iopub.status.busy": "2024-02-21T06:20:04.262525Z",
     "iopub.status.idle": "2024-02-21T06:20:04.269029Z",
     "shell.execute_reply": "2024-02-21T06:20:04.268163Z"
    },
    "papermill": {
     "duration": 0.027355,
     "end_time": "2024-02-21T06:20:04.271059",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.243704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    \n",
    "    def compute_losses(model_, loader):\n",
    "        \"\"\"Auxiliary function to compute per-sample losses\"\"\"\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        all_losses = []\n",
    "\n",
    "        for sample in loader:\n",
    "            images, labels = sample['image'].to(DEVICE), sample['age_group'].to(DEVICE)\n",
    "            logits = model_(images)\n",
    "            \n",
    "            losses = criterion(logits, labels).numpy(force=True)\n",
    "            for l in losses:\n",
    "                all_losses.append(l)\n",
    "\n",
    "        return np.array(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13f21aa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:04.307875Z",
     "iopub.status.busy": "2024-02-21T06:20:04.307528Z",
     "iopub.status.idle": "2024-02-21T06:20:04.316517Z",
     "shell.execute_reply": "2024-02-21T06:20:04.315794Z"
    },
    "papermill": {
     "duration": 0.029516,
     "end_time": "2024-02-21T06:20:04.318460",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.288944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    \n",
    "    # load model with pre-trained weights\n",
    "    model = resnet18(weights=None, num_classes=10)\n",
    "    weights_pretrained = torch.load(test_model_path, map_location=DEVICE)\n",
    "    model.load_state_dict(weights_pretrained)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    retain_losses = compute_losses(model, retain_loader)\n",
    "    forget_losses = compute_losses(model, forget_loader)\n",
    "    test_losses = compute_losses(model, test_loader)\n",
    "    \n",
    "    plt.title(\"Losses on retain, forget and validation set (pre-trained model)\")\n",
    "    plt.hist(retain_losses, density=True, alpha=0.5, bins=50, label=\"Retain set\")\n",
    "    plt.hist(forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "    plt.hist(test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "    plt.xlabel(\"Loss\", fontsize=14)\n",
    "    plt.ylabel(\"Frequency\", fontsize=14)\n",
    "    plt.xlim((0, np.max(test_losses)))\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(frameon=False, fontsize=14)\n",
    "    ax = plt.gca()\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e196143a",
   "metadata": {
    "papermill": {
     "duration": 0.017326,
     "end_time": "2024-02-21T06:20:04.354275",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.336949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As per the above plot, the distributions of losses are quite different between the train and validation sets, as expected. In what follows, we will define an MIA that leverages the fact that examples that were trained on have smaller losses compared to examples that weren't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6900c1",
   "metadata": {
    "papermill": {
     "duration": 0.017123,
     "end_time": "2024-02-21T06:20:04.388983",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.371860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MIA Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61bc314",
   "metadata": {
    "papermill": {
     "duration": 0.017534,
     "end_time": "2024-02-21T06:20:04.424215",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.406681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, we will define an MIA that leverages the fact that examples that were trained on have smaller losses compared to examples that weren't. Using this fact, the simple MIA defined below will aim to infer whether the forget set was in fact part of the training set.\n",
    "\n",
    "This MIA is defined below. It takes as input the per-sample losses of the unlearned model on forget and test examples, and a membership label (0 or 1) indicating which of those two groups each sample comes from. It then returns the cross-validation accuracy of a linear model trained to distinguish between the two classes.\n",
    "\n",
    "Intuitively, an unlearning algorithm is successful with respect to this simple metric if the attacker isn't able to distinguish the forget set from the test set any better than it would for the ideal unlearning algorithm (retraining from scratch without the retain set); see the last part of this MIA section for additional discussion and for computing that reference point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "041326f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:04.461204Z",
     "iopub.status.busy": "2024-02-21T06:20:04.460826Z",
     "iopub.status.idle": "2024-02-21T06:20:04.467595Z",
     "shell.execute_reply": "2024-02-21T06:20:04.466756Z"
    },
    "papermill": {
     "duration": 0.027535,
     "end_time": "2024-02-21T06:20:04.469584",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.442049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    \n",
    "    def simple_mia(sample_loss, members, n_splits=10, random_state=42):\n",
    "        \"\"\"Computes cross-validation score of a membership inference attack.\n",
    "\n",
    "        Args:\n",
    "          sample_loss : array_like of shape (n,).\n",
    "            objective function evaluated on n samples.\n",
    "          members : array_like of shape (n,),\n",
    "            whether a sample was used for training.\n",
    "          n_splits: int\n",
    "            number of splits to use in the cross-validation.\n",
    "        Returns:\n",
    "          scores : array_like of size (n_splits,)\n",
    "        \"\"\"\n",
    "        unique_members = np.unique(members)\n",
    "        if not np.all(unique_members == np.array([0, 1])):\n",
    "            raise ValueError(\"members should only have 0 & 1s\")\n",
    "\n",
    "        attack_model = linear_model.LogisticRegression()\n",
    "        cv = model_selection.StratifiedShuffleSplit(\n",
    "            n_splits=n_splits, random_state=random_state\n",
    "        )\n",
    "        return model_selection.cross_val_score(\n",
    "            attack_model, sample_loss, members, cv=cv, scoring=\"accuracy\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5693a4e2",
   "metadata": {
    "papermill": {
     "duration": 0.017314,
     "end_time": "2024-02-21T06:20:04.504409",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.487095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### MIA on Original Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff830843",
   "metadata": {
    "papermill": {
     "duration": 0.017445,
     "end_time": "2024-02-21T06:20:04.540106",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.522661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As a reference point, we first compute the accuracy of the MIA on the original model to distinguish between the forget set and the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5117ea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:04.576653Z",
     "iopub.status.busy": "2024-02-21T06:20:04.576319Z",
     "iopub.status.idle": "2024-02-21T06:20:04.582407Z",
     "shell.execute_reply": "2024-02-21T06:20:04.581495Z"
    },
    "papermill": {
     "duration": 0.026573,
     "end_time": "2024-02-21T06:20:04.584412",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.557839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    \n",
    "    np.random.seed(42)   # just for reproducibality\n",
    "    \n",
    "    forget_losses = compute_losses(model, forget_loader)\n",
    "    \n",
    "    # Since we have more forget losses than test losses, sub-sample them, to have a class-balanced dataset.\n",
    "    np.random.shuffle(forget_losses)\n",
    "    forget_losses = forget_losses[: len(test_losses)]\n",
    "\n",
    "    samples_mia = np.concatenate((test_losses, forget_losses)).reshape((-1, 1))\n",
    "    labels_mia = [0] * len(test_losses) + [1] * len(forget_losses)\n",
    "\n",
    "    mia_scores = simple_mia(samples_mia, labels_mia)\n",
    "\n",
    "    print(f\"The MIA has an accuracy of {mia_scores.mean():.3f} on forgotten vs unseen images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09cae7",
   "metadata": {
    "papermill": {
     "duration": 0.01697,
     "end_time": "2024-02-21T06:20:04.618806",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.601836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### MIA on Unlearned Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128ad660",
   "metadata": {
    "papermill": {
     "duration": 0.01698,
     "end_time": "2024-02-21T06:20:04.653292",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.636312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We'll now compute the accuracy of the MIA on the unlearned model. We expect the MIA to be less accurate on the unlearned model than on the original model, since the original model has not undergone a procedure to unlearn the forget set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01ea18e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:04.689941Z",
     "iopub.status.busy": "2024-02-21T06:20:04.689274Z",
     "iopub.status.idle": "2024-02-21T06:20:04.695535Z",
     "shell.execute_reply": "2024-02-21T06:20:04.694682Z"
    },
    "papermill": {
     "duration": 0.026683,
     "end_time": "2024-02-21T06:20:04.697446",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.670763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "\n",
    "    net_forget_losses = compute_losses(net, forget_loader)\n",
    "    net_retain_losses = compute_losses(net, retain_loader)\n",
    "    net_test_losses = compute_losses(net, test_loader)\n",
    "    \n",
    "    np.random.shuffle(net_forget_losses)\n",
    "    net_forget_losses = net_forget_losses[: len(test_losses)]\n",
    "\n",
    "    net_samples_mia = np.concatenate((net_test_losses, net_forget_losses)).reshape((-1, 1))\n",
    "    labels_mia = [0] * len(net_test_losses) + [1] * len(net_forget_losses)\n",
    "\n",
    "    net_mia_scores = simple_mia(net_samples_mia, labels_mia)\n",
    "\n",
    "    print(f\"The MIA has an accuracy of {net_mia_scores.mean():.3f} on forgotten vs unseen images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647eaf0",
   "metadata": {
    "papermill": {
     "duration": 0.017314,
     "end_time": "2024-02-21T06:20:04.732621",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.715307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Comparison With Original Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d648b41d",
   "metadata": {
    "papermill": {
     "duration": 0.017114,
     "end_time": "2024-02-21T06:20:04.767473",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.750359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "From the score above, the MIA is indeed less accurate on the unlearned model than on the original model, as expected. Finally, we'll plot the histogram of losses of the unlearned model on the train and validation set. From the below figure, we can observe that the distributions of forget and validation losses are more similar under the unlearned model compared to the original model, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04932994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:04.804060Z",
     "iopub.status.busy": "2024-02-21T06:20:04.803200Z",
     "iopub.status.idle": "2024-02-21T06:20:04.812995Z",
     "shell.execute_reply": "2024-02-21T06:20:04.812117Z"
    },
    "papermill": {
     "duration": 0.030112,
     "end_time": "2024-02-21T06:20:04.814968",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.784856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    ax1.set_title(f\"Pre-trained model.\\nAttack accuracy: {mia_scores.mean():0.2f}\")\n",
    "    ax1.hist(test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "    ax1.hist(forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "    ax1.hist(retain_losses, density=True, alpha=0.5, bins=50, label=\"Retain set\")\n",
    "\n",
    "    ax2.set_title(f\"Unlearned by fine-tuning.\\nAttack accuracy: {net_mia_scores.mean():0.2f}\")\n",
    "    ax2.hist(net_test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "    ax2.hist(net_forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "    ax2.hist(net_retain_losses, density=True, alpha=0.5, bins=50, label=\"Retain set\")\n",
    "\n",
    "    ax1.set_xlabel(\"Loss\")\n",
    "    ax2.set_xlabel(\"Loss\")\n",
    "    ax1.set_ylabel(\"Frequency\")\n",
    "    ax1.set_yscale(\"log\")\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax1.set_xlim((0, np.max(test_losses)))\n",
    "    ax2.set_xlim((0, np.max(test_losses)))\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "    ax1.legend(frameon=False, fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3115c804",
   "metadata": {
    "papermill": {
     "duration": 0.017065,
     "end_time": "2024-02-21T06:20:04.849612",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.832547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Comparison With Trained Model Exclusively on Retain Set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39f4f4",
   "metadata": {
    "papermill": {
     "duration": 0.016936,
     "end_time": "2024-02-21T06:20:04.884075",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.867139",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Since our goal is to approximate the model that has been trained only on the retain set, we'll consider that the gold standard is the score achieved by this model. Intuitively, we expect the MIA accuracy to be around 0.5, since for such a model, both the forget and test set are unseen samples from the same distribution. However, a number of factors such as distribution shift or class imbalance can make this number vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c1c08e",
   "metadata": {
    "papermill": {
     "duration": 0.018245,
     "end_time": "2024-02-21T06:20:04.919548",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.901303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First, we will compute the MIA score on Re-trained model exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2a72fc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:04.955702Z",
     "iopub.status.busy": "2024-02-21T06:20:04.955360Z",
     "iopub.status.idle": "2024-02-21T06:20:04.961177Z",
     "shell.execute_reply": "2024-02-21T06:20:04.960261Z"
    },
    "papermill": {
     "duration": 0.026461,
     "end_time": "2024-02-21T06:20:04.963241",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.936780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "\n",
    "    rt_test_losses = compute_losses(rt_model, test_loader)\n",
    "    rt_forget_losses = compute_losses(rt_model, forget_loader)\n",
    "    rt_retain_losses = compute_losses(rt_model, retain_loader)\n",
    "\n",
    "    rt_samples_mia = np.concatenate((rt_test_losses, rt_forget_losses)).reshape((-1, 1))\n",
    "    labels_mia = [0] * len(rt_test_losses) + [1] * len(rt_forget_losses)\n",
    "    \n",
    "    rt_mia_scores = simple_mia(rt_samples_mia, labels_mia)\n",
    "\n",
    "    print(f\"The MIA has an accuracy of {rt_mia_scores.mean():.3f} on forgotten vs unseen images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100ffb94",
   "metadata": {
    "papermill": {
     "duration": 0.017466,
     "end_time": "2024-02-21T06:20:04.998053",
     "exception": false,
     "start_time": "2024-02-21T06:20:04.980587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, as we've done before, let's compare the histograms of this ideal algorithm (re-trained model) vs the model obtain from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c624129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:05.034583Z",
     "iopub.status.busy": "2024-02-21T06:20:05.034288Z",
     "iopub.status.idle": "2024-02-21T06:20:05.045841Z",
     "shell.execute_reply": "2024-02-21T06:20:05.044985Z"
    },
    "papermill": {
     "duration": 0.032455,
     "end_time": "2024-02-21T06:20:05.047896",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.015441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 6))\n",
    "    \n",
    "    ax1.set_title(f\"Original model.\\nAttack accuracy: {mia_scores.mean():0.2f}\")\n",
    "    ax1.hist(test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "    ax1.hist(forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "    ax1.hist(retain_losses, density=True, alpha=0.5, bins=50, label=\"Retain set\")\n",
    "\n",
    "    ax2.set_title(f\"Re-trained model.\\nAttack accuracy: {rt_mia_scores.mean():0.2f}\")\n",
    "    ax2.hist(rt_test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "    ax2.hist(rt_forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "    ax2.hist(rt_retain_losses, density=True, alpha=0.5, bins=50, label=\"Retain set\")\n",
    "\n",
    "    ax3.set_title(f\"Unlearned by fine-tuning.\\nAttack accuracy: {net_mia_scores.mean():0.2f}\")\n",
    "    ax3.hist(net_test_losses, density=True, alpha=0.5, bins=50, label=\"Test set\")\n",
    "    ax3.hist(net_forget_losses, density=True, alpha=0.5, bins=50, label=\"Forget set\")\n",
    "    ax3.hist(net_retain_losses, density=True, alpha=0.5, bins=50, label=\"Retain set\")\n",
    "\n",
    "    ax1.set_xlabel(\"Loss\")\n",
    "    ax2.set_xlabel(\"Loss\")\n",
    "    ax3.set_xlabel(\"Loss\")\n",
    "    ax1.set_ylabel(\"Frequency\")\n",
    "    ax1.set_yscale(\"log\")\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax3.set_yscale(\"log\")\n",
    "    ax1.set_xlim((0, np.max(test_losses)))\n",
    "    ax2.set_xlim((0, np.max(test_losses)))\n",
    "    ax3.set_xlim((0, np.max(test_losses)))\n",
    "    for ax in (ax1, ax2, ax3):\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "    ax1.legend(frameon=False, fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234019cd",
   "metadata": {
    "papermill": {
     "duration": 0.017144,
     "end_time": "2024-02-21T06:20:05.082986",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.065842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation using Combination of Forgetting Quality, Efficiency and Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940263fc",
   "metadata": {
    "papermill": {
     "duration": 0.017045,
     "end_time": "2024-02-21T06:20:05.117336",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.100291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A metric close to this is **actually used** in the competition<br>\n",
    "**Reference:** https://unlearning-challenge.github.io/assets/data/Machine_Unlearning_Metric.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d94afd",
   "metadata": {
    "papermill": {
     "duration": 0.017477,
     "end_time": "2024-02-21T06:20:05.152233",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.134756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It implements Algorithm 1, the *forgetting quality* $\\mathcal{F}$, and the total scoring function from the above article. It also provides a version of a full scoring function, meant to be mimicking the one used for the competition, and uses it to score a \"perfect\" unlearning algorithm on CIFAR-10.\n",
    "\n",
    "Note that the article does not include all details needed for reproducing the code for computing this metric, thus some gaps are filled in in an improvised manner. The following values and functions are unknown: \n",
    "* the value of $\\delta$,\n",
    "* the summary statistic $f$ that summarises the outpus of the models $R_i$ and $U_i$ into a scalar, and\n",
    "* the attacks that yield the false positive rates (FPRs) and false negative rates (FNRs) needed for Algorithm 1.\n",
    "\n",
    "**The following values are used for this notebook:**\n",
    "* $\\delta = 0.01$\n",
    "* $f = $ cross entropy loss\n",
    "* attacks = [ logistic_regression, best_threshold_attack ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a3dded3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:05.189797Z",
     "iopub.status.busy": "2024-02-21T06:20:05.188914Z",
     "iopub.status.idle": "2024-02-21T06:20:05.193646Z",
     "shell.execute_reply": "2024-02-21T06:20:05.192797Z"
    },
    "papermill": {
     "duration": 0.025348,
     "end_time": "2024-02-21T06:20:05.195543",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.170195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    from copy import deepcopy\n",
    "    from typing import Callable\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "    from sklearn.metrics import make_scorer, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d58d2",
   "metadata": {
    "papermill": {
     "duration": 0.017206,
     "end_time": "2024-02-21T06:20:05.230255",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.213049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "914ba8b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:05.267309Z",
     "iopub.status.busy": "2024-02-21T06:20:05.266493Z",
     "iopub.status.idle": "2024-02-21T06:20:05.278187Z",
     "shell.execute_reply": "2024-02-21T06:20:05.277386Z"
    },
    "papermill": {
     "duration": 0.032476,
     "end_time": "2024-02-21T06:20:05.280121",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.247645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    \n",
    "    def accuracy(net, loader):\n",
    "        \"\"\"Return accuracy on a dataset given by the data loader.\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for sample in loader:\n",
    "            inputs, targets = sample['image'].to(DEVICE), sample['age_group'].to(DEVICE)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        return correct / total\n",
    "\n",
    "\n",
    "    def compute_outputs(net, loader):\n",
    "        \"\"\"Auxiliary function to compute the logits for all datapoints.\n",
    "        Does not shuffle the data, regardless of the loader.\n",
    "        \"\"\"\n",
    "\n",
    "        # Make sure loader does not shuffle the data\n",
    "        if isinstance(loader.sampler, torch.utils.data.sampler.RandomSampler):\n",
    "            loader = DataLoader(\n",
    "                loader.dataset, \n",
    "                batch_size=loader.batch_size, \n",
    "                shuffle=False, \n",
    "                num_workers=loader.num_workers)\n",
    "\n",
    "        all_outputs = []\n",
    "\n",
    "        for sample in loader:\n",
    "            inputs, targets = sample['image'].to(DEVICE), sample['age_group'].to(DEVICE)\n",
    "\n",
    "            logits = net(inputs).detach().cpu().numpy() # (batch_size, num_classes)\n",
    "\n",
    "            all_outputs.append(logits)\n",
    "\n",
    "        return np.concatenate(all_outputs) # (len(loader.dataset), num_classes)\n",
    "\n",
    "\n",
    "    def false_positive_rate(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"Computes the false positive rate (FPR).\"\"\"\n",
    "        fp = np.sum(np.logical_and((y_pred == 1), (y_true == 0)))\n",
    "        n = np.sum(y_true == 0)\n",
    "        return fp / n\n",
    "\n",
    "\n",
    "    def false_negative_rate(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"Computes the false negative rate (FNR).\"\"\"\n",
    "        fn = np.sum(np.logical_and((y_pred == 0), (y_true == 1)))\n",
    "        p = np.sum(y_true == 1)\n",
    "        return fn / p\n",
    "\n",
    "\n",
    "    # The SCORING dictionary is used by sklearn's `cross_validate` function so that\n",
    "    # we record the FPR and FNR metrics of interest when doing cross validation\n",
    "    SCORING = {\n",
    "        'false_positive_rate': make_scorer(false_positive_rate),\n",
    "        'false_negative_rate': make_scorer(false_negative_rate)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b38b8f",
   "metadata": {
    "papermill": {
     "duration": 0.017244,
     "end_time": "2024-02-21T06:20:05.314997",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.297753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The summary statistic $f$\n",
    "\n",
    "Recall that $U^s$ and $R^s$ are the distributions of (scalar) outputs of unlearned and retrained models when receiving a particular forget set example $s \\in S$ as input. That is: \n",
    "\n",
    "$$U^s = \\{ f(U_1(s)), \\dots f(U_N(s)) \\}$$\n",
    "\n",
    "$$R^s = \\{ f(R_1(s)), \\dots f(R_N(s)) \\}$$\n",
    "\n",
    "where $M(s)$ yields the outputs obtained by feeding example s into model $M$, $f$ is the function that transforms those outputs into a scalar, and $N$ is the number of times we retrain / unlearn to obtain an approximation of the distribution of retrained / unlearned models.\n",
    "\n",
    "The actual function $f$ used for scoring is not disclosed in the competition article. Below is an $f$ that computes the cross entropy loss between the logit output x and the highest probability class as given by the model output x. This statistic $f$ corresponds to the (negative log) probability that the model assigns to the class that the model predicts with the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ebef5bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:05.352106Z",
     "iopub.status.busy": "2024-02-21T06:20:05.351250Z",
     "iopub.status.idle": "2024-02-21T06:20:05.357321Z",
     "shell.execute_reply": "2024-02-21T06:20:05.356356Z"
    },
    "papermill": {
     "duration": 0.026799,
     "end_time": "2024-02-21T06:20:05.359375",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.332576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    def cross_entropy_f(x):\n",
    "        # To ensure this function doesn't fail due to nans, find\n",
    "        # all-nan rows in x and substitude them with all-zeros.\n",
    "        x[np.all(np.isnan(x), axis=-1)] = np.zeros(x.shape[-1])\n",
    "\n",
    "        pred = torch.tensor(np.nanargmax(x, axis = -1))\n",
    "        x = torch.tensor(x)\n",
    "\n",
    "        fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "        return fn(x, pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf37314e",
   "metadata": {
    "papermill": {
     "duration": 0.017182,
     "end_time": "2024-02-21T06:20:05.394179",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.376997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Decision rules (attacks)\n",
    "Next, we define the *decision rules (attaks)* to be used when computing the forgetting quality. \n",
    "\n",
    "A decision rule is a function, which takes $U^s$ and $R^s$ as input, learns to predict wheather a (scalar) output $y \\in U^s \\bigcup R^s$ has been generated by an unlearned or a retrained model. The rule than returns the false positive rate (FPR) and the false negative rate (FNR) of the prediction.\n",
    "\n",
    "The original article does not specify what the decision rules used are, so the ones chosen here are improvised:\n",
    "* Logistic regression attack: Train a logistic regression classifier to predict whether a scalar output comes from a retrained or an unlearned model. Use cross validation and return the average FPR and FPN across different test folds.\n",
    "* Best threshold attack: Consider a model that predicts that everything below certain threshold comes from the retrained model, and everything above that threshold comes from the unlearned model. Calculate the FPR and FNR for all the possible thresholds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f82f278",
   "metadata": {
    "papermill": {
     "duration": 0.017249,
     "end_time": "2024-02-21T06:20:05.429108",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.411859",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Logistic Regression Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27b2f46d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:05.466075Z",
     "iopub.status.busy": "2024-02-21T06:20:05.465696Z",
     "iopub.status.idle": "2024-02-21T06:20:05.473193Z",
     "shell.execute_reply": "2024-02-21T06:20:05.472231Z"
    },
    "papermill": {
     "duration": 0.028513,
     "end_time": "2024-02-21T06:20:05.475200",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.446687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    \n",
    "    def logistic_regression_attack(\n",
    "            outputs_U, outputs_R, n_splits=2, random_state=0):\n",
    "        \"\"\"Computes cross-validation score of a membership inference attack.\n",
    "\n",
    "        Args:\n",
    "          outputs_U: numpy array of shape (N)\n",
    "          outputs_R: numpy array of shape (N)\n",
    "          n_splits: int\n",
    "            number of splits to use in the cross-validation.\n",
    "        Returns:\n",
    "          fpr, fnr : float * float\n",
    "        \"\"\"\n",
    "        assert len(outputs_U) == len(outputs_R)\n",
    "\n",
    "        samples = np.concatenate((outputs_R, outputs_U)).reshape((-1, 1))\n",
    "        labels = np.array([0] * len(outputs_R) + [1] * len(outputs_U))\n",
    "\n",
    "        attack_model = linear_model.LogisticRegression()\n",
    "        cv = model_selection.StratifiedShuffleSplit(\n",
    "            n_splits=n_splits, random_state=random_state\n",
    "        )\n",
    "        scores =  model_selection.cross_validate(\n",
    "            attack_model, samples, labels, cv=cv, scoring=SCORING)\n",
    "\n",
    "        fpr = np.mean(scores[\"test_false_positive_rate\"])\n",
    "        fnr = np.mean(scores[\"test_false_negative_rate\"])\n",
    "\n",
    "        return fpr, fnr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08477de1",
   "metadata": {
    "papermill": {
     "duration": 0.017407,
     "end_time": "2024-02-21T06:20:05.510361",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.492954",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Threshold Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3814bdb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:05.547792Z",
     "iopub.status.busy": "2024-02-21T06:20:05.547104Z",
     "iopub.status.idle": "2024-02-21T06:20:05.554844Z",
     "shell.execute_reply": "2024-02-21T06:20:05.553934Z"
    },
    "papermill": {
     "duration": 0.028423,
     "end_time": "2024-02-21T06:20:05.556881",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.528458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    \n",
    "    def best_threshold_attack(\n",
    "            outputs_U: np.ndarray, \n",
    "            outputs_R: np.ndarray, \n",
    "            random_state: int = 0\n",
    "        ) -> tuple[list[float], list[float]]:\n",
    "        \"\"\"Computes FPRs and FNRs for an attack that simply splits into \n",
    "        predicted positives and predited negatives based on any possible \n",
    "        single threshold.\n",
    "\n",
    "        Args:\n",
    "          outputs_U: numpy array of shape (N)\n",
    "          outputs_R: numpy array of shape (N)\n",
    "        Returns:\n",
    "          fpr, fnr : list[float] * list[float]\n",
    "        \"\"\"\n",
    "        assert len(outputs_U) == len(outputs_R)\n",
    "\n",
    "        samples = np.concatenate((outputs_R, outputs_U))\n",
    "        labels = np.array([0] * len(outputs_R) + [1] * len(outputs_U))\n",
    "\n",
    "        N = len(outputs_U)\n",
    "\n",
    "        fprs, fnrs = [], []\n",
    "        for thresh in sorted(list(samples.squeeze())):\n",
    "            ypred = (samples > thresh).astype(\"int\")\n",
    "            fprs.append(false_positive_rate(labels, ypred))\n",
    "            fnrs.append(false_negative_rate(labels, ypred))\n",
    "\n",
    "        return fprs, fnrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c41b00",
   "metadata": {
    "papermill": {
     "duration": 0.017993,
     "end_time": "2024-02-21T06:20:05.592866",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.574873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The privacy degree $\\epsilon_s$, and the scoring function $\\mathcal{H}$\n",
    "\n",
    "Next we specify the function`compute_epsilon_s`, which corresponds to Algorithm 1 from the scoring article. It considers a particular example $s$ from the forget set, it takes the list of FPRs and FNRs as produced by various attacks, and computes the privacy degree $\\epsilon^s$ of the example $s$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4e6d2c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:05.630093Z",
     "iopub.status.busy": "2024-02-21T06:20:05.629282Z",
     "iopub.status.idle": "2024-02-21T06:20:05.639780Z",
     "shell.execute_reply": "2024-02-21T06:20:05.638919Z"
    },
    "papermill": {
     "duration": 0.030916,
     "end_time": "2024-02-21T06:20:05.641715",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.610799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    \n",
    "    def compute_epsilon_s(fpr: list[float], fnr: list[float], delta: float) -> float:\n",
    "        \"\"\"Computes the privacy degree (epsilon) of a particular forget set example, \n",
    "        given the FPRs and FNRs resulting from various attacks.\n",
    "\n",
    "        The smaller epsilon is, the better the unlearning is.\n",
    "\n",
    "        Args:\n",
    "          fpr: list[float] of length m = num attacks. The FPRs for a particular example. \n",
    "          fnr: list[float] of length m = num attacks. The FNRs for a particular example.\n",
    "          delta: float\n",
    "        Returns:\n",
    "          epsilon: float corresponding to the privacy degree of the particular example.\n",
    "        \"\"\"\n",
    "        assert len(fpr) == len(fnr)\n",
    "\n",
    "        per_attack_epsilon = [0.]\n",
    "        for fpr_i, fnr_i in zip(fpr, fnr):\n",
    "            if fpr_i == 0 and fnr_i == 0:\n",
    "                per_attack_epsilon.append(np.inf)\n",
    "            elif fpr_i == 0 or fnr_i == 0:\n",
    "                pass # discard attack\n",
    "            else:\n",
    "                with np.errstate(invalid='ignore'):\n",
    "                    epsilon1 = np.log(1. - delta - fpr_i) - np.log(fnr_i)\n",
    "                    epsilon2 = np.log(1. - delta - fnr_i) - np.log(fpr_i)\n",
    "                if np.isnan(epsilon1) and np.isnan(epsilon2):\n",
    "                    per_attack_epsilon.append(np.inf)\n",
    "                else:\n",
    "                    per_attack_epsilon.append(np.nanmax([epsilon1, epsilon2]))\n",
    "\n",
    "        return np.nanmax(per_attack_epsilon)\n",
    "\n",
    "\n",
    "    def bin_index_fn(\n",
    "            epsilons: np.ndarray, \n",
    "            bin_width: float = 0.5, \n",
    "            B: int = 13\n",
    "            ) -> np.ndarray:\n",
    "        \"\"\"The bin index function.\"\"\"\n",
    "        bins = np.arange(0, B) * bin_width\n",
    "        return np.digitize(epsilons, bins)\n",
    "\n",
    "\n",
    "    def F(epsilons: np.ndarray) -> float:\n",
    "        \"\"\"Computes the forgetting quality given the privacy degrees \n",
    "        of the forget set examples.\n",
    "        \"\"\"\n",
    "        ns = bin_index_fn(epsilons)\n",
    "        hs = 2. / 2 ** ns\n",
    "        return np.mean(hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c2c06",
   "metadata": {
    "papermill": {
     "duration": 0.017142,
     "end_time": "2024-02-21T06:20:05.676271",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.659129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The forgetting quality $\\mathcal{F}$\n",
    "\n",
    "The function below computes the forgetting quality $\\mathcal{F}$ given only the (scalar) outputs of the $N$ retrain and $N$ unlearn models on all the forget set examples. It:\n",
    "1. Iterates over each sample in $S$,\n",
    "2. Performs the attacks specified to obtain lists of FPRs and FNRs,\n",
    "3. Computes the privacy degree $\\epsilon^s$ for each sample,\n",
    "4. Computes the forgetting quality by averaging over the forget scores of all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb5d4b62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:05.713423Z",
     "iopub.status.busy": "2024-02-21T06:20:05.712599Z",
     "iopub.status.idle": "2024-02-21T06:20:05.721570Z",
     "shell.execute_reply": "2024-02-21T06:20:05.720834Z"
    },
    "papermill": {
     "duration": 0.029801,
     "end_time": "2024-02-21T06:20:05.723413",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.693612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    \n",
    "    def forgetting_quality(\n",
    "            outputs_U: np.ndarray, # (N, S)\n",
    "            outputs_R: np.ndarray, # (N, S)\n",
    "            attacks: list[Callable] = [logistic_regression_attack],\n",
    "            delta: float = 0.01\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Both `outputs_U` and `outputs_R` are of numpy arrays of ndim 2:\n",
    "        * 1st dimension coresponds to the number of samples obtained from the \n",
    "          distribution of each model (N=512 in the case of the competition's leaderboard) \n",
    "        * 2nd dimension corresponds to the number of samples in the forget set (S).\n",
    "        \"\"\"\n",
    "\n",
    "        # N = number of model samples\n",
    "        # S = number of forget samples\n",
    "        N, S = outputs_U.shape\n",
    "\n",
    "        assert outputs_U.shape == outputs_R.shape, \\\n",
    "            \"unlearn and retrain outputs need to be of the same shape\"\n",
    "\n",
    "        epsilons = []\n",
    "        pbar = tqdm(range(S))\n",
    "        for sample_id in pbar:\n",
    "            pbar.set_description(\"Computing F...\")\n",
    "\n",
    "            sample_fprs, sample_fnrs = [], []\n",
    "\n",
    "            for attack in attacks: \n",
    "                uls = outputs_U[:, sample_id]\n",
    "                rls = outputs_R[:, sample_id]\n",
    "\n",
    "                fpr, fnr = attack(uls, rls)\n",
    "\n",
    "                if isinstance(fpr, list):\n",
    "                    sample_fprs.extend(fpr)\n",
    "                    sample_fnrs.extend(fnr)\n",
    "                else:\n",
    "                    sample_fprs.append(fpr)\n",
    "                    sample_fnrs.append(fnr)\n",
    "\n",
    "            sample_epsilon = compute_epsilon_s(sample_fprs, sample_fnrs, delta=delta)\n",
    "            epsilons.append(sample_epsilon)\n",
    "\n",
    "        return F(np.array(epsilons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f2f32",
   "metadata": {
    "papermill": {
     "duration": 0.017342,
     "end_time": "2024-02-21T06:20:05.758010",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.740668",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Global scoring function\n",
    "\n",
    "Finally, we package everything together in a single function that takes an unlearning callable, unlearns many times and outputs a dictionary of various quantities, including:\n",
    "* The forget quality $\\mathcal{F}$\n",
    "* The various accuracies $RA^U, TA^U, RA^R, TA^R$\n",
    "* The total score = $\\mathcal{F} \\times \\frac{RA^U}{RA^R} \\times \\frac{TA^U}{TA^R}$\n",
    "\n",
    "Note that since we are provided with only one retrained model for for the CIFAR dataset example, this function includes a littel hack where we add some noise to the output of that pretrained mdoel a few times, instead of pretraining from scratch. The correct version of this function would consider $N$ separate models retrained from scratch with different random seed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb46be37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:05.793946Z",
     "iopub.status.busy": "2024-02-21T06:20:05.793491Z",
     "iopub.status.idle": "2024-02-21T06:20:05.807942Z",
     "shell.execute_reply": "2024-02-21T06:20:05.807027Z"
    },
    "papermill": {
     "duration": 0.034883,
     "end_time": "2024-02-21T06:20:05.809962",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.775079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    \n",
    "    def score_unlearning_algorithm(\n",
    "            data_loaders: dict, \n",
    "            pretrained_models: dict, \n",
    "            net,\n",
    "            n: int = 10,\n",
    "            delta: float = 0.01,\n",
    "            f: Callable = cross_entropy_f,\n",
    "            attacks: list[Callable] = [best_threshold_attack, logistic_regression_attack]\n",
    "            ) -> dict:\n",
    "\n",
    "        # n=512 in the case of unlearn and n=1 in the\n",
    "        # case of retrain, since we are only provided with one retrained model here\n",
    "\n",
    "        retain_loader = data_loaders[\"retain\"]\n",
    "        forget_loader = data_loaders[\"forget\"]\n",
    "        val_loader = data_loaders[\"validation\"]\n",
    "        test_loader = data_loaders[\"testing\"]\n",
    "\n",
    "        original_model = pretrained_models[\"original\"]\n",
    "        rt_model = pretrained_models[\"retrained\"]\n",
    "\n",
    "        outputs_U = []\n",
    "        retain_accuracy = []\n",
    "        test_accuracy = []\n",
    "        forget_accuracy = []\n",
    "\n",
    "        pbar = tqdm(range(n))\n",
    "        for i in pbar:\n",
    "\n",
    "            # unlearned model\n",
    "            pbar.set_description(f\"Unlearning...\")\n",
    "            u_model = net\n",
    "\n",
    "            outputs_Ui = compute_outputs(u_model, forget_loader) \n",
    "            # The shape of outputs_Ui is (len(forget_loader.dataset), 10)\n",
    "            # which for every datapoint is being cast to a scalar using the funtion f\n",
    "            outputs_U.append( f(outputs_Ui) )\n",
    "\n",
    "            pbar.set_description(f\"Computing retain accuracy...\")\n",
    "            retain_accuracy.append(accuracy(u_model, retain_loader))\n",
    "\n",
    "            pbar.set_description(f\"Computing test accuracy...\")\n",
    "            test_accuracy.append(accuracy(u_model, test_loader))\n",
    "\n",
    "            pbar.set_description(f\"Computing forget accuracy...\")\n",
    "            forget_accuracy.append(accuracy(u_model, forget_loader))\n",
    "\n",
    "\n",
    "        outputs_U = np.array(outputs_U) # (n, len(forget_loader.dataset))\n",
    "\n",
    "        assert outputs_U.shape == (n, len(forget_loader.dataset)),\\\n",
    "            \"Wrong shape for outputs_U. Should be (num_model_samples, num_forget_datapoints).\"\n",
    "\n",
    "        RAR = accuracy(rt_model, retain_loader)\n",
    "        TAR = accuracy(rt_model, test_loader)\n",
    "        FAR = accuracy(rt_model, forget_loader)\n",
    "\n",
    "        RAU = np.mean(retain_accuracy)\n",
    "        TAU = np.mean(test_accuracy)\n",
    "        FAU = np.mean(forget_accuracy)\n",
    "\n",
    "        RA_ratio = RAU / RAR\n",
    "        TA_ratio = TAU / TAR\n",
    "\n",
    "        # need to fake this a little because we only have one retrain model\n",
    "        scale = np.std(outputs_U) / 10.\n",
    "        outputs_Ri = compute_outputs(rt_model, forget_loader) #(len(forget_loader.dataset), 10) \n",
    "        outputs_Ri = np.expand_dims(outputs_Ri, axis=0)\n",
    "        outputs_Ri = np.random.normal(\n",
    "            loc=outputs_Ri, scale=scale, size=(n, *outputs_Ri.shape[-2:]))\n",
    "\n",
    "        outputs_R = np.array([ f( oRi ) for oRi in outputs_Ri ])\n",
    "\n",
    "        np.save(\"outputs_U.npy\", outputs_U)\n",
    "        np.save(\"outputs_R.npy\", outputs_R)\n",
    "\n",
    "        f = forgetting_quality(\n",
    "            outputs_U, \n",
    "            outputs_R,\n",
    "            attacks=attacks,\n",
    "            delta=delta)\n",
    "\n",
    "        return {\n",
    "            \"total_score\": f * RA_ratio * TA_ratio,\n",
    "            \"F\": f,\n",
    "            \"unlearn_retain_accuracy\": RAU,\n",
    "            \"unlearn_test_accuracy\": TAU, \n",
    "            \"unlearn_forget_accuracy\": FAU,\n",
    "            \"retrain_retain_accuracy\": RAR,\n",
    "            \"retrain_test_accuracy\": TAR, \n",
    "            \"retrain_forget_accuracy\": FAR,\n",
    "            \"retrain_outputs\": outputs_R,\n",
    "            \"unlearn_outputs\": outputs_U\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e421c0b",
   "metadata": {
    "papermill": {
     "duration": 0.017298,
     "end_time": "2024-02-21T06:20:05.845211",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.827913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Check Our Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca83908e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:05.881591Z",
     "iopub.status.busy": "2024-02-21T06:20:05.880879Z",
     "iopub.status.idle": "2024-02-21T06:20:05.885947Z",
     "shell.execute_reply": "2024-02-21T06:20:05.885106Z"
    },
    "papermill": {
     "duration": 0.025305,
     "end_time": "2024-02-21T06:20:05.887835",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.862530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    \n",
    "    data_loaders = {\n",
    "        \"training\": train_loader,\n",
    "        \"testing\": test_loader,\n",
    "        \"validation\": validation_loader,\n",
    "        \"forget\": forget_loader,\n",
    "        \"retain\": retain_loader\n",
    "    }\n",
    "\n",
    "    pretrained_models = {\n",
    "        \"original\": model,\n",
    "        \"retrained\": rt_model\n",
    "    }\n",
    "\n",
    "    ret = score_unlearning_algorithm(data_loaders, pretrained_models, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e572bac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:05.925662Z",
     "iopub.status.busy": "2024-02-21T06:20:05.925015Z",
     "iopub.status.idle": "2024-02-21T06:20:05.930394Z",
     "shell.execute_reply": "2024-02-21T06:20:05.929559Z"
    },
    "papermill": {
     "duration": 0.027128,
     "end_time": "2024-02-21T06:20:05.932375",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.905247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    col_names = [\"total score\", \"F score\", \"unlearn_retain_accuracy\", \n",
    "                 \"unlearn_test_accuracy\", \"unlearn_forget_accuracy\"]\n",
    "    table=[]\n",
    "    table.append([ret[\"total_score\"], ret[\"F\"], ret[\"unlearn_retain_accuracy\"]*100.0, \n",
    "                  ret[\"unlearn_test_accuracy\"]*100.0, ret[\"unlearn_forget_accuracy\"]*100])\n",
    "    print(tabulate(table, headers=col_names, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a8562ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:05.969012Z",
     "iopub.status.busy": "2024-02-21T06:20:05.968453Z",
     "iopub.status.idle": "2024-02-21T06:20:05.974433Z",
     "shell.execute_reply": "2024-02-21T06:20:05.973570Z"
    },
    "papermill": {
     "duration": 0.026228,
     "end_time": "2024-02-21T06:20:05.976263",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.950035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test:\n",
    "    udata = ret[\"unlearn_outputs\"][:,0]\n",
    "    rdata = ret[\"retrain_outputs\"][:,0]\n",
    "    data = np.array([udata, rdata])\n",
    "\n",
    "    bins = np.arange(np.min(data), np.max(data) + 0.1, 0.1)\n",
    "\n",
    "    plt.hist(udata, bins=bins, alpha=0.7, label=\"Unlearned\")\n",
    "    plt.hist(rdata, bins=bins, alpha=0.7, label=\"Retrained\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a4f2a3",
   "metadata": {
    "papermill": {
     "duration": 0.017061,
     "end_time": "2024-02-21T06:20:06.010672",
     "exception": false,
     "start_time": "2024-02-21T06:20:05.993611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bb171d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:20:06.047495Z",
     "iopub.status.busy": "2024-02-21T06:20:06.046584Z",
     "iopub.status.idle": "2024-02-21T06:20:06.061614Z",
     "shell.execute_reply": "2024-02-21T06:20:06.060845Z"
    },
    "papermill": {
     "duration": 0.03544,
     "end_time": "2024-02-21T06:20:06.063513",
     "exception": false,
     "start_time": "2024-02-21T06:20:06.028073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not test:\n",
    "    \n",
    "    if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n",
    "        # mock submission\n",
    "        subprocess.run('touch submission.zip', shell=True)\n",
    "    else:\n",
    "\n",
    "        # Note: it's really important to create the unlearned checkpoints outside of the working directory \n",
    "        # as otherwise this notebook may fail due to running out of disk space.\n",
    "        # The below code saves them in /kaggle/tmp to avoid that issue.\n",
    "\n",
    "        os.makedirs('/kaggle/tmp', exist_ok=True)\n",
    "        random.seed(42)\n",
    "        retain_loader, forget_loader, validation_loader = get_dataset(64)\n",
    "        net = resnet18(weights=None, num_classes=10)\n",
    "        net.to(DEVICE)\n",
    "        for i in range(512):\n",
    "            net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n",
    "            net_ = unlearning(net, retain_loader, forget_loader, validation_loader)\n",
    "            state = net_.state_dict()\n",
    "            torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n",
    "\n",
    "        # Ensure that submission.zip will contain exactly 512 checkpoints \n",
    "        # (if this is not the case, an exception will be thrown).\n",
    "        unlearned_ckpts = os.listdir('/kaggle/tmp')\n",
    "        if len(unlearned_ckpts) != 512:\n",
    "            raise RuntimeError('Expected exactly 512 checkpoints. The submission will throw an exception otherwise.')\n",
    "\n",
    "        subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6535361,
     "sourceId": 56167,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30554,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.450174,
   "end_time": "2024-02-21T06:20:07.402910",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-21T06:19:53.952736",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
